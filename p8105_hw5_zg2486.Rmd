---
title: "p8105_hw5_zg2486"
output: github_document
date: "2024-11-05"
---

```{r include = FALSE}
library(tidyverse)
library(rvest)

knitr::opts_chunk$set(
	echo = TRUE,
	warning = FALSE,
  fig.width = 6,
  fig.asp = .6,
  out.width = "90%"
)

theme_set(theme_minimal() + theme(legend.position = "bottom"))

options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)

scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d

set.seed(1)
```

# Question1
```{r}
birthday = function(n) {

  bdays = sample(1:365, size = n, replace = TRUE)
  
  duplicate = length(unique(bdays)) < n

  return(duplicate)
}

sim_bday = 
  expand_grid(
    n = 2:50,
    iter = 1:10000
  ) %>% 
  mutate(probability = map(n, birthday)) %>% 
  unnest(probability) %>% 
  group_by(n) %>% 
  summarise(average_prob = mean(probability))

sim_bday %>% 
  ggplot(aes(x = n, y = average_prob)) +
  geom_point(alpha = 0.8) +
  labs(
    title = "Probability of Shared Birthday in a Group as Function of Group Size",
    x = "Group Size (n)",
    y = "Probability of Shared Birthday"
  ) 
```
From the plot, we can see the probability that at least two people in the group will share a birthday by averaging across the 10000 simulation runs increases as the sample group size become larger.



# Question 2
```{r}
library(broom)
sim_mean_sd = function(n = 30, mu, sigma = 5) {
  
  sim_df = 
    tibble(
      x = rnorm(n, mean = mu, sd = sigma)
    )

  t_test_result = t.test(pull(sim_df, x), mu = 0) %>% 
    tidy()

  out_df = 
    sim_df %>%  
    summarize(
      mu_hat = mean(x),
      p_value = pull(t_test_result, p.value)
    )
  
  return(out_df)
}

sim_data = 
  expand_grid(
    mu = 0:6,
    iter = 1:5000
  ) %>% 
  mutate(reject_null = map(mu, sim_mean_sd, n = 30)) %>% 
  unnest(reject_null) %>% 
  mutate(sig_indicator = as.numeric(p_value < 0.05))
```

#### Plot for Power vs True Mean.
```{r}
power_plot = 
  sim_data %>% 
  group_by(mu) %>% 
  summarise(power = mean(p_value < 0.05)) %>% 
  ggplot(aes(x = mu, y = power)) + 
  geom_point() +
  geom_line() +
  labs(
    title = "Power of the One-Sample t-Test vs True Mean (Effect Size)",
    x = "True Mean (mu)",
    y = "Power"
  )

power_plot
```
The plot shows that power increases as the true mean (𝜇) increases, because as the true effect size become larger, the probability of failing to reject the null hypothesis when it is false (Type II error) decreases, leading to higher power.

#### Plot of Average estimates of 𝜇̂and 𝜇̂ only in samples for which the null was rejected vs True mean
```{r}
average_mu_hat = 
  sim_data %>% 
  group_by(mu) %>% 
  summarise(
    avg_mu_hat = mean(mu_hat),
    avg_mu_hat_reject = mean(mu_hat[p_value < 0.05])
    ) %>% 
  ggplot(aes(x = mu)) + 
  geom_point(aes(y = avg_mu_hat, color = "All")) +
  geom_line(aes(y = avg_mu_hat, color = "All")) +
  geom_point(aes(y = avg_mu_hat_reject, color = "Rejected")) +
  geom_line(aes(y = avg_mu_hat_reject, color = "Rejected")) +
  labs(
    title = "Average estimate of mu_hat vs True Mean (mu)",
    x = "True Mean (mu)",
    y = "Average estimate of mu_hat",
    color = "Scale"
  )

average_mu_hat
```
The average estimate of 𝜇̂ across tests for which the null is rejected is not equal to the true mean (mu) when effect size is small.

```{r}
sim_data %>% 
  ggplot(aes(x = mu_hat)) +
  geom_histogram() +
  facet_grid(mu ~ sig_indicator) +
  labs(
    title = "Distribution of 𝜇̂ across different levels of μ and significance indicator",
    x = "mu_hat",
    y = "Counts"
  ) +
  theme_minimal()
```
We can explain why the average of 𝜇̂ across tests for which the null is rejected is higher than the true mean of 𝜇， because for small true 𝜇, the variability in 𝜇̂ dominates, and rejection are are influenced by chance. This leads to average of 𝜇̂ is far from the true mean (𝜇). As the 𝜇increases, the power increases, and the tests better detect the true effect, leading to the average 𝜇̂ be closer to the true mean (𝜇).



# Question 3
```{r}
homicide_df = 
  read_csv("./data/homicide-data.csv") %>% 
  mutate(
    city_state = str_c(city, state, sep = ","),
    resolution = case_when(
      disposition == "Closed without arrest" ~ "unsolved",
      disposition == "Open/No arrest"        ~ "unsolved",
      disposition == "Closed by arrest"      ~ "solved")
  ) %>%
  filter(city_state != "Tulsa,AL")

homicide_by_states =
  homicide_df %>% 
  group_by(city_state) %>% 
  summarise(
    total_homicides = n(),
    unsolved_homicides = sum(resolution == "unsolved")
  )
```
The raw data is relatively completed, with few missing value. I noticed that there is a city "Tulsa" that exist in 2 states, therefore, I will analyze the data for each city based on the new created variable `city_state`. And there is only 1 observation in "Tulsa, AL", so I will drop this.

#### Estimate the proportion of homicides that are unsolved for city of Baltimore, MD
```{r}
prop.test(
  homicide_by_states %>% filter(city_state == "Baltimore,MD") %>% pull(unsolved_homicides), 
  homicide_by_states %>% filter(city_state == "Baltimore,MD") %>% pull(total_homicides)) %>%
  broom::tidy() %>% 
  knitr::kable()
```

#### Create a tidy dataframe with estimated proportions and CIs for each city
```{r}
each_city_df = 
  homicide_by_states %>% 
  mutate(
    prop_test_results = map2(
      .x = unsolved_homicides, .y = total_homicides, ~prop.test(x = .x, n = .y)),
    tidy_results = map(.x = prop_test_results, ~broom::tidy(.x))
  ) %>% 
  unnest(tidy_results) %>% 
  select(city_state, estimate, conf.low, conf.high)
```

#### Estimate the proportion of homicides that are unsolved for each city
```{r}
each_city_df %>% 
  mutate(city_state = fct_reorder(city_state, estimate)) %>% 
  ggplot(aes(x = city_state, y = estimate)) +
  geom_point() + 
  geom_errorbar(aes(ymin = conf.low, ymax = conf.high)) + 
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1)) +
  labs(
    title = "Estimates and CIs for each city",
    x = "city_state",
    y = "Estimate with 95% CI"
  )
```
Richmond,VA and Chicago,IL has the lowest and higher proportion of unsolved homicides, respectively. Tampa,FL and Savanmah,GA have a wider range of 95% confidence interval, reflecting greater uncertainty about where the true proportion of unsolved homicides lie. 
